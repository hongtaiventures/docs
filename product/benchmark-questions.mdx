---
title: "Benchmark Questions"
description: "Benchmark Questions are category-neutral question sets used to test your brand's AI visibility. CiteScore generates them from your Brand Profile or lets you create them manually."
---

## Overview

Benchmark Questions are the questions CiteScore uses to test your AI visibility. Each question is designed to mirror how real users ask AI assistants about products and services in your category — without mentioning your brand name.

A standard benchmark set contains 20 questions. When you run an [AEO Audit](/product/aeo-audits), these questions are sent to an AI model, and the responses are analyzed for whether your brand appears.

## Why Neutral Questions Matter

Benchmark questions intentionally omit your brand name. This tests organic visibility — whether AI naturally recommends you when users ask category-relevant questions.

**Good benchmark question:** "What are the best analytics tools for SaaS startups?"

**Not a benchmark question:** "Is [Your Brand] a good analytics tool?"

The first question tests whether AI thinks of you without prompting. The second only tests whether AI can describe you when asked directly. Organic discovery is what matters for real-world AI visibility.

## Question Types

CiteScore generates questions across four categories to provide comprehensive coverage:

| Type | Example | What It Tests |
|------|---------|--------------|
| **Category queries** | "What are the best [category] tools?" | Category association |
| **Problem queries** | "How do I solve [problem]?" | Problem-solution positioning |
| **Use-case queries** | "What's good for [specific use case]?" | Use-case relevance |
| **Audience queries** | "What should [persona] use for [need]?" | Audience-specific visibility |

### Intent Stages

Each question is also tagged with an intent stage:

- **Awareness** — broad category exploration ("What is [category]?")
- **Consideration** — evaluating options ("What are the best tools for...?")
- **Decision** — making a choice ("Which [category] tool should I choose?")
- **Retention** — using the product ("How do I get more value from my [category] tool?")

## Generating Benchmark Sets

To create a benchmark question set:

1. Navigate to your brand's **Benchmarks** section
2. Click **Generate Questions**
3. CiteScore analyzes your Brand Profile and generates 20 questions
4. Review and optionally edit the questions
5. Save the set

You can also add questions manually or create multiple sets for different purposes (e.g., one set focused on your core category, another on a specific use case).

## Running Benchmark Tests

Benchmark tests run individual questions through AI models to check for your brand:

1. Select a benchmark set
2. Run tests against one or more AI models
3. For each question, CiteScore records:
   - Whether your brand was mentioned
   - Your position (Primary, Secondary, Listed, or Not Mentioned)
   - Which competitors were mentioned
   - The full AI response

Test results feed into your [AEO Score](/reference/scoring-methodology) calculation and provide the per-question detail needed to identify specific visibility gaps.

## Interpreting Results

Look for patterns in your benchmark results:

- **Questions where you are consistently mentioned** — your strong positions; maintain them
- **Questions where competitors appear but you don't** — priority content opportunities
- **Questions where no one in your category appears** — potential first-mover opportunities
- **Questions with inconsistent results across models** — areas where your visibility is fragile

## FAQ

<AccordionGroup>
  <Accordion title="How many questions should a benchmark set have?">
    The standard is 20 questions, which provides enough data points for meaningful scoring while remaining manageable to analyze. CiteScore's AEO Score is calibrated around 20-question sets (20 questions x 5 max points = 100-point scale).
  </Accordion>
  <Accordion title="Can I edit the AI-generated questions?">
    Yes. After generation, you can edit, add, or remove questions before saving the set. You can also create entirely manual question sets if you prefer full control.
  </Accordion>
  <Accordion title="How often should I update my benchmark questions?">
    Keep your core question set consistent for trend tracking. Update when your product positioning changes significantly or when you want to test new category angles. Consider maintaining a stable "core" set and a separate "exploratory" set.
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="AEO Audits" icon="magnifying-glass-chart" href="/product/aeo-audits">
    Run a full audit using your benchmark questions.
  </Card>
  <Card title="Scoring Methodology" icon="chart-simple" href="/reference/scoring-methodology">
    Understand how benchmark results translate to your AEO Score.
  </Card>
</CardGroup>
